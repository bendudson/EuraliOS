* Memory management

  In the [[file:02-userspace.org][last section]] we worked out how to run a program in ring 3,
but couldn't protect programs from each other or run more than one
without having to manually choose memory ranges for each program.

To isolate programs from each other we need to set up separate
page tables for each process. We will want to keep the kernel
pages mapped, and add the user pages.

First add a new function to =memory.rs= which creates a new empty
level 4 pagetable for the user process:
#+BEGIN_SRC rust
fn create_empty_pagetable() -> (*mut PageTable, u64) {
    // Need to borrow as mutable so that we can allocate new frames
    // and so modify the frame allocator
    let memory_info = unsafe {MEMORY_INFO.as_mut().unwrap()};

    // Get a frame to store the level 4 table
    let level_4_table_frame = memory_info.frame_allocator.allocate_frame().unwrap();
    let phys = level_4_table_frame.start_address(); // Physical address
    let virt = memory_info.physical_memory_offset + phys.as_u64(); // Kernel virtual address
    let page_table_ptr: *mut PageTable = virt.as_mut_ptr();

    // Clear all entries in the page table
    unsafe {
        (*page_table_ptr).zero();
    }

    (page_table_ptr, phys.as_u64())
}
#+END_SRC
Note that it returns both a pointer (i.e. a virtual memory address) to
the new page table, and also the physical address of the table. That
physical address is what needs to be written to the CR3 register in
order for this page table to be used.

If we switch to this page table then none of the kernel code or data
will be accessible. Before using this page table we therefore want to
add all the kernel pages which are in the currently active page table.
The simplest, brute force, way to do this is to create a new set of
page tables for each process. That avoids any possibility of pages
being made available to multiple processes, but duplicates the kernel
page tables. In =memory.rs= the =MemoryInfo= struct can be extended to
include a reference to the kernel page table:
#+BEGIN_SRC rust
  struct MemoryInfo {
    boot_info: &'static BootInfo,
    physical_memory_offset: VirtAddr,
    frame_allocator: BootInfoFrameAllocator,
    kernel_l4_table: &'static mut PageTable // new
  }
#+END_SRC
which can be set in the =init= function:
#+begin_src rust
  pub fn init(boot_info: &'static BootInfo) {
      // ...
      // Store boot_info for later calls
      unsafe { MEMORY_INFO = Some(MemoryInfo {
          boot_info,
          physical_memory_offset,
          frame_allocator,
          kernel_l4_table: level_4_table // new
      }) };
      // ...
  }
#+end_src


In the =new_user_thread= function in =process.rs= we can now get the
page table pointer and physical address, and then switch to the new
table:
#+BEGIN_SRC rust
  if let Ok(obj) = object::File::parse(bin) {
      let (user_page_table_ptr, user_page_table_physaddr) =
          memory::create_kernel_only_pagetable(); // New
      unsafe {
          asm!("mov cr3, {addr}", addr = in(reg) user_page_table_physaddr); // New
      }
      ...
#+END_SRC
So when memory is allocated and the ELF data is read, the new page
table entries are in the new page table. This writes the physical
address of the page table to CR3 (Control Register 3), which triggers
a TLB flush. Since we are not changing the kernel pages, these can be
kept rather than flushed by setting the Global bit in the page table,
as explained in [[https://wiki.osdev.org/TLB][this OSdev wiki page]].

The user program should now run with the new page table, producing the
same output as before!  Unfortunately this pagetable is now used for
all threads, and creating a new user process will change the page
table for all threads. To really separate processes we need to change
page tables during context switches, which brings us to discussing
processes and threads.

** Single-threaded programs

   At this point we need to make some choices which will affect the kind of
kernel that we end up with. The simplest solution for now is to assign each
thread its own page table, so each program is single-threaded. To do this,
in =processes.rs= add a field to the =Thread= struct:
#+begin_src rust
  struct Thread {
    /// Thread ID
    tid: usize,

    /// Page table physical address
    page_table_physaddr: u64, // New
    ...
  }
#+end_src

In the =new_kernel_thread= function we'll set this address to
zero, to indicate that no page table switch is needed, since
kernel pages are mapped in all tables.
#+begin_src rust
  Box::new(Thread {
      tid: 0,
      page_table_physaddr: 0, // New
#+end_src

In the =new_user_thread= we'll store the physical address of the
new page table:
#+begin_src rust
  Box::new(Thread {
      tid: 0,
      page_table_physaddr: user_page_table_physaddr, // New
#+end_src

We're going to need to switch page tables in a couple of places now
(the context switch and new user thread code) so let's define
a function in =memory.rs= to do this:
#+begin_src rust
  pub fn switch_to_pagetable(physaddr: u64) {
      unsafe {
          asm!("mov cr3, {addr}",
               addr = in(reg) physaddr);
      }
  }
#+end_src
And add =use core::arch::asm;= near the top of =memory.rs=.  We can
then call this function in =new_user_thread=, replacing the unsafe asm
block:
#+begin_src rust
  memory::switch_to_pagetable(user_page_table_physaddr);
#+end_src
At this point it's also very important to consider interrupts in our
=new_user_thread= function: It is changing to a new page table and
then modifying it.  If a context switch occurs while this is
happening, the page table will be switched and changes will be made to
the wrong tables. We can either disable interrupts while working with the
new page table, or the context switch needs to save and restore each
thread's page table.

Finally in =process.rs= the function =schedule_next=, which is called
by the timer interrupt to switch context, can be modified:
#+begin_src rust
  match current_thread.as_ref() {
      Some(thread) => {
          gdt::set_interrupt_stack_table(
              gdt::TIMER_INTERRUPT_INDEX as usize,
              VirtAddr::new(thread.kernel_stack_end));

          if thread.page_table_physaddr != 0 {
              memory::switch_to_pagetable(thread.page_table_physaddr); // New
          }
          thread.context as usize
#+end_src
An optimisation here would be to only switch pagetable if it's
different from the already active pagetable e.g if there is only one
running thread.

*** Two user programs

To try this out we need to run two userspace programs simultaneously.
In =main.rs= we have the entry point:
#+begin_src rust
  entry_point!(kernel_entry);

  fn kernel_entry(boot_info: &'static BootInfo) -> ! {
      blog_os::init();
      memory::init(boot_info);
      syscalls::init();

      #[cfg(test)]
      test_main();

      process::new_kernel_thread(kernel_thread_main);

      blog_os::hlt_loop();
  }
#+end_src
which sets up some basic kernel functions, then starts a kernel
thread and waits for it to be scheduled. At this point we go
to the =kernel_thread_main= function, and can launch two
of the same programs:
#+begin_src rust
  fn kernel_thread_main() {
      println!("Kernel thread start");

      process::new_user_thread(include_bytes!("../user/hello"));
      process::new_user_thread(include_bytes!("../user/hello"));

      blog_os::hlt_loop();
  }
#+end_src

To see if both threads are running side-by-side, we can add some delays
between outputs in each thread. For now this will be just brute force =nop= loops
in =hello.rs=:
#+begin_src rust
  #[no_mangle]
  pub unsafe extern "sysv64" fn _start() -> ! {
      print!("Hello from user world! {}", 42);

      for i in 1..10 {
          println!("{}", i);
          for i in 1..10000000 { // wait
              unsafe { asm!("nop");}
          }
      }

      loop {}
  }
#+end_src
When run you should see two counters interleaved, each counting up to 9.

** Multi-threaded programs

Having only one thread per program is straightforward but restrictive.
Multi-threaded programs can perform tasks while waiting for events, or
service many requests efficiently while sharing memory between
threads.

A common way to make a program multi-threaded is something like =fork=
(or =clone= syscall for threads in Linux): One thread performs the syscall,
and two threads return. Typically the only difference between the threads is
the return value of the =fork= call: One thread gets a return value of 0, and
the other gets a non-zero value (e.g. a thread ID, or stack location).

There are three things needed:
1. Give each thread a unique ID
2. Create a separate stack for each thread. This will require finding
   space in virtual memory for new stacks, rather than the fixed
   address used currently.
3. We need to have a user thread enter the kernel, copy its context, and
   have that context restored by a context switch during a timer interrupt.
   The simplest way to do that is to make the kernel stack during a syscall
   look the same as it does during a timer interrupt.

*** Unique thread ID

This doesn't necessarily need to be done now, but it'll be useful to label
threads somehow. First we need a way to generate unique IDs. In
=process.rs= we can add a counter:
#+begin_src rust
  lazy_static! {
      // ...
      static ref UNIQUE_COUNTER: RwLock<u64> = RwLock::new(0);
  }
#+end_src
and then a function which returns a different number each time
it is called:
#+begin_src rust
pub fn unique_id() -> u64 {
    interrupts::without_interrupts(|| {
        let mut counter = UNIQUE_COUNTER.write();
        *counter += 1;
        *counter
    })
}
#+end_src

Everywhere we create a new =Thread= object we can now write:
#+begin_src rust
  Box::new(Thread {
      tid: unique_id(), // new
#+end_src

*** Thread stack allocation

Whle forked (created) threads will share a page table, it's important
that they have separate stacks. Currently we use a fixed (virtual)
address for the user stack (=const USER_STACK_START: u64 =
0x5200000;=) which we hope doesn't overlap with data loaded from
the ELF file.

To do this we'll need to understand a bit better how the virtual
memory is being used. These python routines are useful, as they convert
between virtual addresses and page table indices.
#+begin_src python
def page_table_indices(vaddr):
    return ((vaddr >> 39) & 511, (vaddr >> 30) & 511, (vaddr >> 21) & 511, (vaddr >> 12) & 511, vaddr & 4095)

def page_table_address(indices):
    return (indices[0] << 39) + (indices[1] << 30) + (indices[2] << 21) + (indices[3] << 12) + indices[4]
#+end_src

We're currently loading our user programs starting at 0x5000000,
corresponding to indices =(0, 0, 40, 0, 0)= so level 4 and level 3
table index 0, level 2 index 40, level 1 index 0 and frame offset 0.

A common choice is to use the lower half of 32-bit address space for
user code, and reserve the upper half between 0x80000000 (page table
indices =(0, 2, 0, 0, 0)=) and 0x100000000 (indices =(0, 4, 0, 0,
0)=)for kernel code. To add a little security to our ELF user code
loader we can define a range of allowed addresses in =process.rs=
#+begin_src rust
  const USER_CODE_START: u64 = 0x5000000;
  const USER_CODE_END: u64 = 0x80000000;
#+end_src
then before allocating memory in =new_user_thread= we can check that
the memory is in the allowed range, returning an error if it is
not. Remember to change page table back before returning. We should
also free the new page tables, but haven't added functions to do that
yet.
#+begin_src rust
  if (start_address < VirtAddr::new(USER_CODE_START))
      || (end_address >= VirtAddr::new(USER_CODE_END)) {
          memory::switch_to_pagetable(original_page_table);
          return Err("ELF segment outside allowed range");
      }
#+end_src


Above 0x100000000 we can use any address range we
like for things like stack and heap allocations. A small part of this
huge address space is already used:

- The bootloader maps all memory starting at address =0x18000000000=,
  corresponding to page table indices =(3, 0, 0, 0, 0)=.
- The kernel
  heap (in =allocator.rs=) starts at =0x_4444_4444_0000= which is page
  index =(136, 273, 34, 64, 0)=.

For now we can reserve a level 1 page table, which has 512 pages. If
we allocate 8 to each thread then we'll be able to have up to 64
threads per page table. We can divide this up into (from low to high):
- One unused page table as a guard; accessing this (user stack
  overflow) will trigger a fault.
- Seven user stack pages.

A user stack underflow accesses the unused guard page in the next set
of thread pages.

We can choose an arbitrary page table, for example =(5,0,0,*,*)= which
maps virtual memory addresses 0x28000000000 to 0x28000200000.



*** Create Context struct in syscall

When a thread fork syscall is made, a new thread context must
be made, which is the same as the original thread. The easiest
way to do this is to capture a Context in syscall in the same
way that we do in a timer interrupt. In the process we're going
to stop using the user stack to store the state.

[[https://github.com/redox-os/kernel/blob/master/src/arch/x86_64/interrupt/syscall.rs#L65][Redox OS]] syscall handler


[[https://wiki.osdev.org/Task_State_Segment][Task State Segment]] layout

[[https://www.felixcloutier.com/x86/wrmsr][wrmsr]] and [[https://www.felixcloutier.com/x86/swapgs][swapgs]] instructions


